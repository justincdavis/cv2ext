"""
This type stub file was generated by pyright.
"""

import pickle

"""
This is a modified version of the cloudpickle module.
Patches:
- https://github.com/numba/numba/pull/7388
  Avoid resetting class state of dynamic classes.

Original module docstring:

Pickler class to extend the standard pickle.Pickler functionality

The main objective is to make it natural to perform distributed computing on
clusters (such as PySpark, Dask, Ray...) with interactively defined code
(functions, classes, ...) written in notebooks or console.

In particular this pickler adds the following features:
- serialize interactively-defined or locally-defined functions, classes,
  enums, typevars, lambdas and nested functions to compiled byte code;
- deal with some other non-serializable objects in an ad-hoc manner where
  applicable.

This pickler is therefore meant to be used for the communication between short
lived Python processes running the same version of Python and libraries. In
particular, it is not meant to be used for long term storage of Python objects.

It does not include an unpickler, as standard Python unpickling suffices.

This module was extracted from the `cloud` package, developed by `PiCloud, Inc.
<https://web.archive.org/web/20140626004012/http://www.picloud.com/>`_.

Copyright (c) 2012-now, CloudPickle developers and contributors.
Copyright (c) 2012, Regents of the University of California.
Copyright (c) 2009 `PiCloud, Inc. <https://web.archive.org/web/20140626004012/http://www.picloud.com/>`_.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    * Neither the name of the University of California, Berkeley nor the
      names of its contributors may be used to endorse or promote
      products derived from this software without specific prior written
      permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""
DEFAULT_PROTOCOL = ...
_PICKLE_BY_VALUE_MODULES = ...
_DYNAMIC_CLASS_TRACKER_BY_CLASS = ...
_DYNAMIC_CLASS_TRACKER_BY_ID = ...
_DYNAMIC_CLASS_TRACKER_LOCK = ...
_DYNAMIC_CLASS_TRACKER_REUSING = ...
PYPY = ...
builtin_code_type = ...
if PYPY:
    builtin_code_type = ...
_extract_code_globals_cache = ...
def register_pickle_by_value(module): # -> None:
    """Register a module to make it functions and classes picklable by value.

    By default, functions and classes that are attributes of an importable
    module are to be pickled by reference, that is relying on re-importing
    the attribute from the module at load time.

    If `register_pickle_by_value(module)` is called, all its functions and
    classes are subsequently to be pickled by value, meaning that they can
    be loaded in Python processes where the module is not importable.

    This is especially useful when developing a module in a distributed
    execution environment: restarting the client Python process with the new
    source code is enough: there is no need to re-install the new version
    of the module on all the worker nodes nor to restart the workers.

    Note: this feature is considered experimental. See the cloudpickle
    README.md file for more details and limitations.
    """
    ...

def unregister_pickle_by_value(module): # -> None:
    """Unregister that the input module should be pickled by value."""
    ...

def list_registry_pickle_by_value(): # -> set[Any]:
    ...

STORE_GLOBAL = ...
DELETE_GLOBAL = ...
LOAD_GLOBAL = ...
GLOBAL_OPS = ...
HAVE_ARGUMENT = ...
EXTENDED_ARG = ...
_BUILTIN_TYPE_NAMES = ...
def is_tornado_coroutine(func): # -> Any | Literal[False]:
    """Return whether `func` is a Tornado coroutine function.

    Running coroutines are not supported.
    """
    ...

def subimport(name): # -> ModuleType:
    ...

def dynamic_subimport(name, vars): # -> ModuleType:
    ...

def instance(cls):
    """Create a new instance of a class.

    Parameters
    ----------
    cls : type
        The class to create an instance of.

    Returns
    -------
    instance : cls
        A new instance of ``cls``.
    """
    ...

@instance
class _empty_cell_value:
    """Sentinel for empty closures."""
    @classmethod
    def __reduce__(cls): # -> str:
        ...
    


_DATACLASSE_FIELD_TYPE_SENTINELS = ...
class Pickler(pickle.Pickler):
    _dispatch_table = ...
    dispatch_table = ...
    def dump(self, obj): # -> None:
        ...
    
    def __init__(self, file, protocol=..., buffer_callback=...) -> None:
        ...
    
    if not PYPY:
        dispatch = ...
        def reducer_override(self, obj): # -> tuple[type[type], tuple[None]] | tuple[type[type], tuple[ellipsis]] | tuple[type[type], tuple[_NotImplementedType]] | tuple[Callable[..., type[type] | Any], tuple[Any]] | tuple[Callable[..., Any], tuple[tuple[type, ...], str, str, dict[str, Any], str, str | Any, None], tuple[dict[Any, Any], dict[Any, Any]], None, None, Callable[..., Any]] | tuple[Callable[..., Any | type], tuple[Any, Any, Any, dict[Any, Any], str | Any, None], tuple[dict[Any, Any], dict[Any, Any]], None, None, Callable[..., Any]] | _NotImplementedType | tuple[Callable[..., FunctionType], tuple[Any, Any, None, None, tuple[_Cell | Any, ...] | None], tuple[Any, dict[str, Any]], None, None, Callable[..., None]]:
            """Type-agnostic reducing callback for function and classes.

            For performance reasons, subclasses of the C `pickle.Pickler` class
            cannot register custom reducers for functions and classes in the
            dispatch_table attribute. Reducers for such types must instead
            implemented via the special `reducer_override` method.

            Note that this method will be called for any object except a few
            builtin-types (int, lists, dicts etc.), which differs from reducers
            in the Pickler's dispatch_table, each of them being invoked for
            objects of a specific type only.

            This property comes in handy for classes: although most classes are
            instances of the ``type`` metaclass, some of them can be instances
            of other custom metaclasses (such as enum.EnumMeta for example). In
            particular, the metaclass will likely not be known in advance, and
            thus cannot be special-cased using an entry in the dispatch_table.
            reducer_override, among other things, allows us to register a
            reducer that will be called for any class, independently of its
            type.

            Notes:

            * reducer_override has the priority over dispatch_table-registered
            reducers.
            * reducer_override can be used to fix other limitations of
              cloudpickle for other types that suffered from type-specific
              reducers, such as Exceptions. See
              https://github.com/cloudpipe/cloudpickle/issues/248
            """
            ...
        
    else:
        dispatch = ...
        def save_global(self, obj, name=..., pack=...): # -> None:
            """Main dispatch method.

            The name of this method is somewhat misleading: all types get
            dispatched here.
            """
            ...
        
        def save_function(self, obj, name=...): # -> None:
            """Registered with the dispatch to handle all function types.

            Determines what kind of function obj is (e.g. lambda, defined at
            interactive prompt, etc) and handles the pickling appropriately.
            """
            ...
        
        def save_pypy_builtin_func(self, obj): # -> None:
            """Save pypy equivalent of builtin functions.

            PyPy does not have the concept of builtin-functions. Instead,
            builtin-functions are simple function instances, but with a
            builtin-code attribute.
            Most of the time, builtin functions should be pickled by attribute.
            But PyPy has flaky support for __qualname__, so some builtin
            functions such as float.__new__ will be classified as dynamic. For
            this reason only, we created this special routine. Because
            builtin-functions are not expected to have closure or globals,
            there is no additional hack (compared the one already implemented
            in pickle) to protect ourselves from reference cycles. A simple
            (reconstructor, newargs, obj.__dict__) tuple is save_reduced.  Note
            also that PyPy improved their support for __qualname__ in v3.6, so
            this routing should be removed when cloudpickle supports only PyPy
            3.6 and later.
            """
            ...
        


def dump(obj, file, protocol=..., buffer_callback=...): # -> None:
    """Serialize obj as bytes streamed into file

    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to
    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication
    speed between processes running the same Python version.

    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure
    compatibility with older versions of Python (although this is not always
    guaranteed to work because cloudpickle relies on some internal
    implementation details that can change from one Python version to the
    next).
    """
    ...

def dumps(obj, protocol=..., buffer_callback=...): # -> bytes:
    """Serialize obj as a string of bytes allocated in memory

    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to
    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication
    speed between processes running the same Python version.

    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure
    compatibility with older versions of Python (although this is not always
    guaranteed to work because cloudpickle relies on some internal
    implementation details that can change from one Python version to the
    next).
    """
    ...

CloudPickler = Pickler
