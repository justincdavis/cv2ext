"""
This type stub file was generated by pyright.
"""

from contextlib import contextmanager

'''
Implements the cuda module as called from within an executing kernel
(@cuda.jit-decorated function).
'''
class Dim3:
    '''
    Used to implement thread/block indices/dimensions
    '''
    def __init__(self, x, y, z) -> None:
        ...
    
    def __str__(self) -> str:
        ...
    
    def __repr__(self): # -> LiteralString:
        ...
    
    def __iter__(self): # -> Generator[Any, Any, None]:
        ...
    


class GridGroup:
    '''
    Used to implement the grid group.
    '''
    def sync(self): # -> None:
        ...
    


class FakeCUDACg:
    '''
    CUDA Cooperative Groups
    '''
    def this_grid(self): # -> GridGroup:
        ...
    


class FakeCUDALocal:
    '''
    CUDA Local arrays
    '''
    def array(self, shape, dtype): # -> NDArray[bool_ | object_ | void]:
        ...
    


class FakeCUDAConst:
    '''
    CUDA Const arrays
    '''
    def array_like(self, ary):
        ...
    


class FakeCUDAShared:
    '''
    CUDA Shared arrays.

    Limitations: assumes that only one call to cuda.shared.array is on a line,
    and that that line is only executed once per thread. i.e.::

        a = cuda.shared.array(...); b = cuda.shared.array(...)

    will erroneously alias a and b, and::

        for i in range(10):
            sharedarrs[i] = cuda.shared.array(...)

    will alias all arrays created at that point (though it is not certain that
    this would be supported by Numba anyway).
    '''
    def __init__(self, dynshared_size) -> None:
        ...
    
    def array(self, shape, dtype): # -> NDArray[bool_ | object_ | void]:
        ...
    


addlock = ...
sublock = ...
andlock = ...
orlock = ...
xorlock = ...
maxlock = ...
minlock = ...
compare_and_swaplock = ...
caslock = ...
inclock = ...
declock = ...
exchlock = ...
class FakeCUDAAtomic:
    def add(self, array, index, val):
        ...
    
    def sub(self, array, index, val):
        ...
    
    def and_(self, array, index, val):
        ...
    
    def or_(self, array, index, val):
        ...
    
    def xor(self, array, index, val):
        ...
    
    def inc(self, array, index, val):
        ...
    
    def dec(self, array, index, val): # -> Literal[0]:
        ...
    
    def exch(self, array, index, val):
        ...
    
    def max(self, array, index, val):
        ...
    
    def min(self, array, index, val):
        ...
    
    def nanmax(self, array, index, val):
        ...
    
    def nanmin(self, array, index, val):
        ...
    
    def compare_and_swap(self, array, old, val):
        ...
    
    def cas(self, array, index, old, val):
        ...
    


class FakeCUDAFp16:
    def hadd(self, a, b):
        ...
    
    def hsub(self, a, b):
        ...
    
    def hmul(self, a, b):
        ...
    
    def hdiv(self, a, b):
        ...
    
    def hfma(self, a, b, c):
        ...
    
    def hneg(self, a):
        ...
    
    def habs(self, a):
        ...
    
    def hsin(self, x): # -> Any:
        ...
    
    def hcos(self, x): # -> Any:
        ...
    
    def hlog(self, x): # -> Any:
        ...
    
    def hlog2(self, x): # -> Any:
        ...
    
    def hlog10(self, x): # -> Any:
        ...
    
    def hexp(self, x): # -> Any:
        ...
    
    def hexp2(self, x): # -> Any:
        ...
    
    def hexp10(self, x): # -> float16:
        ...
    
    def hsqrt(self, x): # -> Any:
        ...
    
    def hrsqrt(self, x): # -> float16:
        ...
    
    def hceil(self, x): # -> Any:
        ...
    
    def hfloor(self, x): # -> Any:
        ...
    
    def hrcp(self, x): # -> Any:
        ...
    
    def htrunc(self, x): # -> Any:
        ...
    
    def hrint(self, x): # -> Any:
        ...
    
    def heq(self, a, b):
        ...
    
    def hne(self, a, b):
        ...
    
    def hge(self, a, b):
        ...
    
    def hgt(self, a, b):
        ...
    
    def hle(self, a, b):
        ...
    
    def hlt(self, a, b):
        ...
    
    def hmax(self, a, b):
        ...
    
    def hmin(self, a, b):
        ...
    


class FakeCUDAModule:
    '''
    An instance of this class will be injected into the __globals__ for an
    executing function in order to implement calls to cuda.*. This will fail to
    work correctly if the user code does::

        from numba import cuda as something_else

    In other words, the CUDA module must be called cuda.
    '''
    def __init__(self, grid_dim, block_dim, dynshared_size) -> None:
        ...
    
    @property
    def cg(self): # -> FakeCUDACg:
        ...
    
    @property
    def local(self): # -> FakeCUDALocal:
        ...
    
    @property
    def shared(self): # -> FakeCUDAShared:
        ...
    
    @property
    def const(self): # -> FakeCUDAConst:
        ...
    
    @property
    def atomic(self): # -> FakeCUDAAtomic:
        ...
    
    @property
    def fp16(self): # -> FakeCUDAFp16:
        ...
    
    @property
    def threadIdx(self):
        ...
    
    @property
    def blockIdx(self):
        ...
    
    @property
    def warpsize(self): # -> Literal[32]:
        ...
    
    @property
    def laneid(self):
        ...
    
    def syncthreads(self): # -> None:
        ...
    
    def threadfence(self): # -> None:
        ...
    
    def threadfence_block(self): # -> None:
        ...
    
    def threadfence_system(self): # -> None:
        ...
    
    def syncthreads_count(self, val):
        ...
    
    def syncthreads_and(self, val):
        ...
    
    def syncthreads_or(self, val):
        ...
    
    def popc(self, val): # -> int:
        ...
    
    def fma(self, a, b, c):
        ...
    
    def cbrt(self, a):
        ...
    
    def brev(self, val): # -> int:
        ...
    
    def clz(self, val): # -> int:
        ...
    
    def ffs(self, val): # -> int:
        ...
    
    def selp(self, a, b, c):
        ...
    
    def grid(self, n): # -> tuple[Any, Any] | tuple[Any, Any, Any]:
        ...
    
    def gridsize(self, n): # -> tuple[Any, Any] | tuple[Any, Any, Any]:
        ...
    


@contextmanager
def swapped_cuda_module(fn, fake_cuda_module): # -> Generator[None, Any, None]:
    ...

